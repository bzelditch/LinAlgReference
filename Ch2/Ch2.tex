\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{epic}
\usepackage{eepic}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage{wrapfig}

\title{Chapter 2}
\author{Ben Zelditch}

\begin{document}

\maketitle

\section*{Linear Transformations}

Linear transformations (or maps) are the most important topic in linear algebra. In fact, linear algebra basically is the study of linear transformations between vector spaces. There are a few different ways one can think about them and we'll look at all of them in this chapter. 

\subsection*{Linear Transformations}

Let's first define what a linear transformation is. Let $V, W$ be two vector spaces. A linear transformation $T : V \rightarrow W$ is a function with the following two properties:

\vskip 3mm

\textbf{Additivity}

$T(u + v) = T(u) + T(v)$ for all $u, v \in V$

\vskip 2mm

\textbf{Homogeneity}

$T(\alpha v) = \alpha T(v)$ for all $\alpha \in \mathbb{R}$ and $v \in V$

\vskip 5mm

That's it. Pretty simple, right? Note that homogeneity implies that $T(0) = 0$ for \textit{any} linear map $T$ because

$$T(0) = T(0v) = 0T(v) = 0.$$

From now on we'll use the notations $T(v)$ and $Tv$ interchangeably. Most textbooks do this already, but it's worth mentioning again that they mean the same thing to clear up any confusion.

\vskip 2mm

The set of all linear maps from $V$ to $W$ is denoted $\mathcal{L}(V, W)$. The set of all linear maps from a vector space $V$ to itself is denoted $\mathcal{L}(V) = \mathcal{L}(V, V)$ and is called the set of all linear \textit{operators} on $V$. Linear operators are a very important special case of linear transformations that we'll look into later.

Once again, let $T \in \mathcal{L}(V, W)$ be a linear map. Let $v \in V$ be a vector and let $b_1, b_2, \ldots, b_n$ be a basis of $V$.  Remember from Chapter 1 that we can we can write

$$v = a_1 b_1 + a_2 b_2 + \ldots + a_n b_n$$

for some $a_1, a_2, \ldots, a_n \in \mathbb{R}$. By the properties of linear transformations that we looked at above, this implies that

$$Tv = T(a_1 b_1 + a_2 b_2 + \ldots + a_n b_n) = a_1 T b_1 + a_2 T b_2 + \ldots + a_n T b_n.$$

This is huge. This fact is telling us that a linear map $T : V \rightarrow W$ is \textit{uniquely} determined by where it maps the basis vectors. Better yet, we can \textit{define} a linear transformation simply by defining where the transformation maps the basis vectors. Since any other vector in the vector space can be written uniquely in terms of the basis vectors, its image under the transformation must be uniquely determined as well. It's that easy. 

\subsection*{Linear Transformations as Vector Spaces}

Let $S, T \in \mathcal{L} (V, W)$ be two linear maps. We can make $\mathcal{L}(V, W)$ into a vector space by defining addition and scalar multiplication on it. Let's define $S + T \in \mathcal{L}(V, W)$ to be 

$$(S + T)v = Sv + Tv$$

for $v \in V$ and scalar multiplication to be

$$(\alpha T)v = \alpha (Tv)$$

for $\alpha \in \mathbb{R}$. With these two definitions alone, we can now think of $\mathcal{L} (V, W)$ as a vector space. Going a step further, we can define the \textit{product} of two linear maps as well. First let $U$ be a vector space. Then for $T \in \mathcal{L} (U, V)$ and $S \in \mathcal{L} (V, W)$ we have

$$(ST)u = S(Tu)$$

for $u \in U$. In other words, $ST$ is just the usual composition $S \circ T$. When $S$ and $T$ are both linear, so is $ST$. Note that linear map multiplication is associative and distributive, but \textit{not} commutative. In general, $ST \neq TS$.

\subsection*{Null Spaces and Ranges}

Let $T \in \mathcal{L} (V, W)$. The \textit{null space} of $T$, denoted $\textrm{null} \: T$, is the subset of $V$ consisting of those vectors that $T$ maps to 0:

$$\textrm{null} \: T = \{ v \in V : Tv = 0 \}.$$

The null space is also called the \textit{kernel}. As an example, suppose that $T : \mathbb{R}^2 \rightarrow \mathbb{R}^2$ is the map that shifts a two-dimensional vector backward:

$$T( (x_1, x_2)^T ) = (x_2, 0)^T.$$

Then $\textrm{null} \: T = \{ (x_1, 0)^T : x_1 \in \mathbb{R} \}$, i.e. the null space of $T$ is the $x$-axis. Observe that $\textrm{null} \: T$ is a subspace of $\mathbb{R}^2$. In fact, this is always true! For any linear map $T \in \mathcal{L} (V, W)$, we have that $\textrm{null} \: T \subseteq V$ is a subspace of $V$.

\vskip 3mm

For $T \in \mathcal{L} (V, W)$ we define the \textit{range} of $T$, denoted $\textrm{range} \: T$, to be the subset of $W$ consisting of vectors of the form $Tv$ for some $v \in V$:

$$\textrm{range} \: T = \{ Tv : v \in V \}.$$

Sometimes the word \textit{image} is used instead of range, but they mean the same thing. For example, if we take $T$ to be the backwards shift map that we looked at above, then
$\textrm{range} \: T = \{ (x_2, 0)^T : x_2 \in \mathbb{R} \}.$
In other words, the range of $T$ is the $x$-axis. At first glance this may seem confusing because the null space of $T$ is also the $x$-axis, but remember:  the range of a linear map $T$ is a subset of the \textit{co-domain} and the null space is a subset of the \textit{domain}. Similar to the null space, we have that $\textrm{range} \: T \subseteq W$ is a subspace of $W$.

\vskip 4mm
Now that we're familiar with null spaces and ranges, we can state a few facts about them. Let's look at the most important one first. For $T \in \mathcal{L} (V, W)$ we have

$$\textrm{dim} \: V = \textrm{dim} \: \textrm{null} \: T + \textrm{dim} \: \textrm{range} \: T.$$

Surprising, isn't it? At first glance it isn't clear why the dimension of the domain of a linear map should be related to the dimension of the codomain in such a simple way, but it's true. In fact, the proof of this theorem isn't very difficult! It relies solely on the properties of bases that we've looked at here. 

For a sanity check, let's look at the backwards shift map again. Recall that the domain of the map was $\mathbb{R}^2$, so its dimension was two. In addition, we determined that the null space and the range were the $x$-axis, each of which was one dimensional. Since $2 = 1 + 1$, the theorem holds.

The reason this theorem is so useful is because we often know two of the terms in the equality, but not the other (typically we don't know the dimension of the kernel), so it allows us to quickly find the third. It also allows us to deduce additional facts about linear maps. For example, suppose that $V$ and $W$ are finite-dimensional vector spaces such that $\textrm{dim} \: V > \textrm{dim} \: W$. Then \textit{no} linear map from $V$ to $W$ is one-to-one. This is a pretty strong statement, but its proof is remarkably straightforward given the theorem. Here it is: 

\vskip 3mm
\begin{proof}
Let $T \in \mathcal{L} (V, W)$. Then

$$ \textrm{dim} \:  \textrm{null}  \: T = \textrm{dim} V - \textrm{dim} \: \textrm{range} \: T$$

$$ \geq \textrm{dim} \: V - \textrm{dim} \: W$$

$$ > 0.$$

The second line follows from the fact that $\textrm{range} \: T \subseteq W$ so $\textrm{dim} \: \textrm{range} \: T \leq \textrm{dim} \: W$. Since $ \textrm{dim} \:  \textrm{null}  \: T > 0$, we know that there exist vectors other than the zero vector that get mapped to zero, so $T$ can't be one-to-one.
\end{proof}

Using this theorem, we proved a very strong statement about linear transformations as a whole in just three lines. That's pretty cool.

\subsection*{The Matrix of a Linear Map}

Matrices:  the bane of every engineering student's existence at one point or another. Up until now we've been looking at linear algebra through the lens of abstract linear transformations and, as a result, we haven't been able to work through many concrete examples. To use what we've learned about linear maps in a real-world setting we need matrices. Let's quickly run through the basics of matrices before going any further. An $m \times n$ matrix $A$ is an $m \times n$ rectangular grid of real numbers that looks like this:

$$A = 
\begin{pmatrix} 
a_{11} & a_{12} & \ldots & a_{1n} \\
\vdots  & \ddots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{pmatrix}.$$

We'll use $a_{ij}$ to denote the entry of $A$ in the $i$th row and $j$th column. Recall that we can add and multiply matrices (assuming the dimensions work out) and that we can multiply a vector by a matrix. Let's look at how these work. If we take $A$ to be the matrix above and let $v = (v_1, v_2, \ldots, v_n)^T$ be a vector in $\mathbb{R}^n$ then
 
\[ Av = \begin{pmatrix} 
a_{11} & a_{12} & \ldots & a_{1n} \\
\vdots  & \ddots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{pmatrix}
%
\begin{pmatrix}
v_1 \\
v_2 \\
\vdots \\
v_n
\end{pmatrix} = 
%
v_1  \begin{pmatrix}
a_{11} \\
a_{21} \\
\vdots \\
a_{m1}
\end{pmatrix} +
%
v_2  \begin{pmatrix}
a_{12} \\
a_{22} \\
\vdots \\
a_{m2}
\end{pmatrix} +
\ldots +
%
v_n  \begin{pmatrix}
a_{1n} \\
a_{2n} \\
\vdots \\
a_{mn}
\end{pmatrix} .\]

Matrix multiplication is more cumbersome to write out completely, but if $A$ is an $m \times n$ matrix and $B$ is an $n \times k$ matrix then $AB$ is an $m \times k$ whose entry in the $i$th row and $j$th column is

$$ab_{ij} = \sum_{k = 1}^n a_{ik}b_{kj}.$$




\end{document}