\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{epic}
\usepackage{eepic}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage{wrapfig}

\title{Chapter 1}
\author{Ben Zelditch}

\begin{document}

\maketitle

\section*{Vector Spaces}

Linear algebra is the study of linear transformations (or maps) between finite-dimensional vector spaces. Although slightly intimidating at first, this is the
right the way to define the subject. Let's break this definition down.

\subsection*{Vectors and Fields}

Before we can think about vector spaces and linear transformations, we need to define vectors. A vector $v = (v_1, v_2, \ldots, v_n)$ over a field $\mathbb{F}$ is an ordered collection of elements of $\mathbb{F}$. In other words, each $v_i$ in the vector $v$ belongs to the field $\mathbb{F}$. There are many different examples of fields, but the most common ones are the real numbers $\mathbb{R}$ and the complex numbers $\mathbb{C}$. We'll use $\mathbb{R}$ as our field in this reference because it's the most intuitive for most people, but it doesn't really matter which one you choose. The elements $v_i \in \mathbb{F}$ of the field are called scalars.

\subsection*{Vector Spaces}

Now that we've defined vectors, we can now define vector spaces. A vector space $V$ is a set of vectors $v$ over a field $\mathbb{F}$ with an addition operation and a scalar multiplication operation. This means that for any pair of vectors $x, y \in V$ in our vector space, there exists another vector $x + y \in V$ that is result of "adding" $x$ and $y$. The word "add" is in quotation marks because the addition operation doesn't have to be the usual addition operation that we normally think of, but a more general operation that satisfies the following properties:

\vskip 1mm

$\textbf{\textrm{Commutativity:}}$

$ x + y = y + x \: \textrm{for all} \: x, y \in V$

\vskip 2mm

$\textbf{\textrm{Associativity:}}$ 

$ (x + y) + z = x + (y + z) \: \textrm{for all} \: x, y, z \in V$

\vskip 2mm

$\textbf{\textrm{Additive Identity:}} $

$\: \textrm{There exists a vector} \: 0 \in V \: \textrm{such that} \: v + 0 = v \: \textrm{for all} \: v \in V$

\vskip 2mm

$\textbf{\textrm{Additive Inverse:}} $

$\: \textrm{For every} \: v \in V \: \textrm{there exists another vector} \: w \in V \: \textrm{such that} \: v + w = 0$

\vskip 2mm

$\textbf{\textrm{Multiplicative Identity:}} $

$\: \textrm{There exists a field element} \: 1 \in \mathbb{F} \: \textrm{such that} \: 1v = v \: \textrm{for all} \: v \in V$

\vskip 2mm

$\textbf{\textrm{Distributive Properties:}} $

$\: \alpha(x + y) = \alpha x + \alpha y \: \textrm{and} \: (\alpha + \beta)x = \alpha x + \beta x \: \textrm{for all} \: \alpha, \beta \in \mathbb{F} \: \textrm{and} \: x, y \in V$


\vskip 5mm
This is the most general definition for vector space addition, but clearly it isn't very useful for practical purposes. From now on we'll stick to the usual definitions of addition and scalar multiplication, taking $\mathbb{R}$ as our field and $\mathbb{R}^n$ as our vector space. So let $x = (x_1, x_2, \ldots, x_n)$ and $y = (y_1, y_2, \ldots, y_n)$ be vectors in $\mathbb{R}^n$ and let $\alpha \in \mathbb{R}$ be a real number. Then

$$x + y = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \: \textrm{and} \: \alpha x = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n).$$

The additive identity $0 \in \mathbb{R}^n$ is $(0, 0 \ldots, 0)$ and the multiplicative identity $1 \in \mathbb{R}$ is 1, of course. 

Before moving on, there are a few properties of vector spaces that are basic, but fundamental, and we won't prove them here. The first is that the additive identity $0$ is unique. The second is that every vector $x$ in the vector space has a unique additive inverse, denoted $-x$. The third is that $-1(x) = -x$ for all $x$ in the vector space.

\subsection*{Subspaces}

A subspace $U$ of the vector space $V$ is a subset $U \subseteq V$ that is itself a vector space. For example, if $V = \mathbb{R}^2$ then one possible subspace of $V$ is $U = \{ (x_1, 0) : x_1 \in \mathbb{R} \}$. It isn't hard to see that $U$ satisfies all of the vector space properties listed above, so it definitely qualifies as a subspace of $V$. 

\subsection*{Sums and Direct Sums}

Another important concept is the addition of two vector spaces. If $U$ and $V$ are two vector spaces, then we define the \textit{sum} of $U$ and $V$ to be $U + V = \{ u + v : \: u \in U, v \in V \}$. For example, if $U = \{ (x_1, 0) : x_1 \in \mathbb{R} \}$ and $V = \{ (0, x_2) : x_2 \in \mathbb{R} \}$, then $U + V = \{ (x_1, 0) + (0, x_2) : x_1 \in \mathbb{R}, x_2 \in \mathbb{R} \} = \{ (x_1, x_2) : x_1 \in \mathbb{R}, x_2 \in \mathbb{R} \} = \mathbb{R}^2$. Observe from this example that we can now decompose a vector space into the sum of two other vector spaces. This is a very useful tool that we'll use later. Finally, we define the \textit{direct sum} of two vector spaces. We say that a vector space $W$ is the direct sum of two vector spaces $U$ and $V$ if and only if $W = U + V$ \textit{and} $U \cap V = \{0\}$. This is written as $W = U \oplus V$. In fact, the example we just looked at is an example of a direct sum! Therefore we can write $\mathbb{R}^2 = \{ (x_1, 0) : x_1 \in \mathbb{R} \} \oplus \{ (0, x_2) : x_2 \in \mathbb{R} \}$. More on this later.

\subsection*{Linear Independence and Span}

Let $v_1, v_2, \ldots, v_n \in \mathbb{R}^n$ be a set of vectors. A \textit{linear combination} of these vectors is a new vector of the form $a_1 v_1 + a_2 v_2 + \ldots + a_n v_n$, where $a_1, a_2, \ldots, a_n \in \mathbb{R}$ are some real numbers. The \textit{span} of the vectors $v_1, v_2, \ldots, v_n $ is defined to be the set of all linear combinations of the vectors, i.e.

$$\textrm{span} (v_1, v_2, \ldots, v_n) = \{ a_1 v_1 + a_2 v_2 + \ldots + a_n v_n : a_1, a_2, \ldots, a_n \in \mathbb{R}\}.$$ 

The span of a set of vectors is \textit{always} a vector space. If a particular vector space $V$ is equal to the span of a set of vectors $v_1, v_2, \ldots, v_n$ then we say that $v_1, v_2, \ldots, v_n$ span $V$. Observe that the vectors $e_1 = (1, 0)$ and $e_2 = (0, 1)$ span $\mathbb{R}^2$ because $\textrm{span} (e_1, e_2) = \{a_1 e_1 + a_2 e_2 : a_1, a_2 \in \mathbb{R}\} = \{(a_1, a_2) : a_1, a_2 \in \mathbb{R}\} = \mathbb{R}^2$. Also, a vector space is \textit{finite dimensional} if some finite set of vectors spans it. All examples of vector spaces that we'll look at here will be finite dimensional.

Shifting gears, let $v_1, v_2, \ldots, v_n \in \mathbb{R}^n$ be the set of vectors we introduced earlier and suppose that $v \in \textrm{span}(v_1, v_2, \ldots, v_n)$. By definition, this means that we can write $v = a_1 v_1 + a_2 v_2 + \ldots + a_n v_n$ for some numbers $a_1, a_2, \ldots, a_n \in \mathbb{R}$. 

But are the numbers $a_1, a_2, \ldots, a_n$ unique? In other words, is there another way to write $v$ in terms of the vectors $v_1, v_2, \ldots, v_n$? Suppose that there is, so we can write $v = \hat{a}_1 v_1 + \hat{a}_2 v_2 + \ldots + \hat{a}_n v_n$. Then subtracting our two equations for $v$ we get

$$0 = (a_1 - \hat{a}_1) v_1 + (a_2 - \hat{a}_2) v_2 + \ldots + (a_n - \hat{a}_n) v_n.$$

Clearly this equation holds when $a_1 - \hat{a}_1 = a_2 - \hat{a}_2 = \ldots = a_n - \hat{a}_n = 0$. If this is the only way to satisfy this equation then we say that the vectors $v_1, v_2, \ldots, v_n$ are \textit{linearly independent}. Note that this implies that $a_i = \hat{a}_i$ for all $i$ so there is only one way to write $v$. To reiterate, a set of vectors $v_1, v_2, \ldots, v_n$ is linearly independent if and only if $a_1 v_1 + a_2 v_2 + \ldots + a_n v_n = 0$ implies that $a_1 = a_2 = \ldots = a_n = 0$. Observe that the vectors $e_1$ and $e_2$ that we looked at before are linearly independent because

$$a_1 e_1 + a_2 e_2 = (a_1, 0) + (0, a_2) = (a_1, a_2) = (0, 0)$$

immediately implies that $a_1 = a_2 = 0$. On the other hand, if we let $v_1 = (1, 2)$ and $v_2 = (3, 6)$ then

$$a_1 v_1 + a_2 v_2 = a_1 (1, 2) + a_2 (3, 6) = (0, 0)$$

also holds with $a_1 = -3$ and $a_2 = 1$. Therefore $v_1$ and $v_2$ are \textit{not} linearly independent. Of course, setting $a_1 = a_2 = 0$ would also work as it always does.

Intuitively, a set of vectors is (linearly) independent if they all point in mutually orthogonal directions. One can think of each independent vector in the set as providing another dimension to the span. In the case where $v_1 = (1, 2)$ and $v_2 = (3, 6)$ above, the vector $v_1$ and $v_2$ point in the same direction, so $v_2$ does not provide another dimension to their span (or vice versa with $v_1$). 

Linear independence is another one of those fundamental concepts of linear algebra and it comes up again and again. If its definition seems strange and its motivation unclear then that's totally OK. It will make more sense when we continue with bases.

\subsection*{Bases}



\subsection*{Summary}

So there you have it! Not too complicated, right? Vector spaces are just a collection of vectors equipped with a few important properties. Now that we're comfortable working with them, we can move on to the heart of linear algebra:  linear transformations.

\end{document}