\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{epic}
\usepackage{eepic}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage{wrapfig}

\title{Chapter 1}
\author{Ben Zelditch}

\begin{document}

\maketitle

\section*{Vector Spaces}

Linear algebra is the study of linear transformations (or maps) between finite-dimensional vector spaces. Although slightly intimidating at first, this is the
right the way to define the subject. Let's break this definition down.

\subsection*{Vectors and Fields}

Before we can think about vector spaces and linear transformations, we need to define vectors. A vector $v = (v_1, v_2, \ldots, v_n)$ over a field $\mathbb{F}$ is an ordered collection of elements of $\mathbb{F}$. In other words, each $v_i$ in the vector $v$ belongs to the field $\mathbb{F}$. There are many different examples of fields, but the most common ones are the real numbers $\mathbb{R}$ and the complex numbers $\mathbb{C}$. We'll use $\mathbb{R}$ as our field in this reference because it's the most intuitive for most people, but it doesn't really matter which one you choose. The elements $v_i \in \mathbb{F}$ of the field are called scalars.

\subsection*{Vector Spaces}

Now that we've defined vectors, we can now define vector spaces. A vector space $V$ is a set of vectors $v$ over a field $\mathbb{F}$ with an addition operation and a scalar multiplication operation. This means that for any pair of vectors $x, y \in V$ in our vector space, there exists another vector $x + y \in V$ that is result of "adding" $x$ and $y$. The word "add" is in quotation marks because the addition operation doesn't have to be the usual addition operation that we normally think of, but a more general operation that satisfies the following properties:

\vskip 1mm

$\textbf{\textrm{Commutativity:}}$

$ x + y = y + x \: \textrm{for all} \: x, y \in V$

\vskip 2mm

$\textbf{\textrm{Associativity:}}$ 

$ (x + y) + z = x + (y + z) \: \textrm{for all} \: x, y, z \in V$

\vskip 2mm

$\textbf{\textrm{Additive Identity:}} $

$\: \textrm{There exists a vector} \: 0 \in V \: \textrm{such that} \: v + 0 = v \: \textrm{for all} \: v \in V$

\vskip 2mm

$\textbf{\textrm{Additive Inverse:}} $

$\: \textrm{For every} \: v \in V \: \textrm{there exists another vector} \: w \in V \: \textrm{such that} \: v + w = 0$

\vskip 2mm

$\textbf{\textrm{Multiplicative Identity:}} $

$\: \textrm{There exists a field element} \: 1 \in \mathbb{F} \: \textrm{such that} \: 1v = v \: \textrm{for all} \: v \in V$

\vskip 2mm

$\textbf{\textrm{Distributive Properties:}} $

$\: \alpha(x + y) = \alpha x + \alpha y \: \textrm{and} \: (\alpha + \beta)x = \alpha x + \beta x \: \textrm{for all} \: \alpha, \beta \in \mathbb{F} \: \textrm{and} \: x, y \in V$


\vskip 5mm
This is the most general definition for vector space addition, but clearly it isn't very useful for practical purposes. From now on we'll stick to the usual definitions of addition and scalar multiplication, taking $\mathbb{R}$ as our field and $\mathbb{R}^n$ as our vector space. So let $x = (x_1, x_2, \ldots, x_n)$ and $y = (y_1, y_2, \ldots, y_n)$ be vectors in $\mathbb{R}^n$ and let $\alpha \in \mathbb{R}$ be a real number. Then

$$x + y = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \: \textrm{and} \: \alpha x = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n).$$

The additive identity $0 \in \mathbb{R}^n$ is $(0, 0 \ldots, 0)$ and the multiplicative identity $1 \in \mathbb{R}$ is 1, of course. 

Before moving on, there are a few properties of vector spaces that are basic, but fundamental, and we won't prove them here. The first is that the additive identity $0$ is unique. The second is that every vector $x$ in the vector space has a unique additive inverse, denoted $-x$. The third is that $-1(x) = -x$ for all $x$ in the vector space.

\subsection*{Subspaces}

A subspace $U$ of the vector space $V$ is a subset $U \subseteq V$ that is itself a vector space. For example, if $V = \mathbb{R}^2$ then one possible subspace of $V$ is $U = \{ (x_1, 0) : x_1 \in \mathbb{R} \}$. It isn't hard to see that $U$ satisfies all of the vector space properties listed above, so it definitely qualifies as a subspace of $V$. 

\subsection*{Sums and Direct Sums}

Another important concept is the addition of two vector spaces. If $U$ and $V$ are two vector spaces, then we define the \textit{sum} of $U$ and $V$ to be $U + V = \{ u + v : \: u \in U, v \in V \}$. For example, if $U = \{ (x_1, 0) : x_1 \in \mathbb{R} \}$ and $V = \{ (0, x_2) : x_2 \in \mathbb{R} \}$, then $U + V = \{ (x_1, 0) + (0, x_2) : x_1 \in \mathbb{R}, x_2 \in \mathbb{R} \} = \{ (x_1, x_2) : x_1 \in \mathbb{R}, x_2 \in \mathbb{R} \} = \mathbb{R}^2$. Observe from this example that we can now decompose a vector space into the sum of two other vector spaces. This is a very useful tool that we'll use later. Finally, we define the \textit{direct sum} of two vector spaces. We say that a vector space $W$ is the direct sum of two vector spaces $U$ and $V$ if and only if $W = U + V$ \textit{and} $U \cap V = \{0\}$. This is written as $W = U \oplus V$. In fact, the example we just looked at is an example of a direct sum! Therefore we can write $\mathbb{R}^2 = \{ (x_1, 0) : x_1 \in \mathbb{R} \} \oplus \{ (0, x_2) : x_2 \in \mathbb{R} \}$. More on this later.

\subsection*{Linear Independence and Span}

Let $v_1, v_2, \ldots, v_n \in \mathbb{R}^n$ be a set of vectors. A \textit{linear combination} of these vectors is a new vector of the form $a_1 v_1 + a_2 v_2 + \ldots + a_n v_n$, where $a_1, a_2, \ldots, a_n \in \mathbb{R}$ are some real numbers. The \textit{span} of the vectors $v_1, v_2, \ldots, v_n $ is defined to be the set of all linear combinations of the vectors, i.e.

$$\textrm{span} (v_1, v_2, \ldots, v_n) = \{ a_1 v_1 + a_2 v_2 + \ldots + a_n v_n : a_1, a_2, \ldots, a_n \in \mathbb{R}\}.$$ 

The span of a set of vectors is \textit{always} a vector space. If a particular vector space $V$ is equal to the span of a set of vectors $v_1, v_2, \ldots, v_n$ then we say that $v_1, v_2, \ldots, v_n$ span $V$. Observe that the vectors $e_1 = (1, 0)$ and $e_2 = (0, 1)$ span $\mathbb{R}^2$ because $\textrm{span} (e_1, e_2) = \{a_1 e_1 + a_2 e_2 : a_1, a_2 \in \mathbb{R}\} = \{(a_1, a_2) : a_1, a_2 \in \mathbb{R}\} = \mathbb{R}^2$. Also, a vector space is \textit{finite dimensional} if some finite set of vectors spans it. All examples of vector spaces that we'll look at here will be finite dimensional.

Shifting gears, let $v_1, v_2, \ldots, v_n \in \mathbb{R}^n$ be the set of vectors we introduced earlier and suppose that $v \in \textrm{span}(v_1, v_2, \ldots, v_n)$. By definition, this means that we can write $v = a_1 v_1 + a_2 v_2 + \ldots + a_n v_n$ for some numbers $a_1, a_2, \ldots, a_n \in \mathbb{R}$. 

But are the numbers $a_1, a_2, \ldots, a_n$ unique? In other words, is there another way to write $v$ in terms of the vectors $v_1, v_2, \ldots, v_n$? Suppose that there is, so we can write $v = \hat{a}_1 v_1 + \hat{a}_2 v_2 + \ldots + \hat{a}_n v_n$. Then subtracting our two equations for $v$ we get

$$0 = (a_1 - \hat{a}_1) v_1 + (a_2 - \hat{a}_2) v_2 + \ldots + (a_n - \hat{a}_n) v_n.$$

Clearly this equation holds when $a_1 - \hat{a}_1 = a_2 - \hat{a}_2 = \ldots = a_n - \hat{a}_n = 0$. If this is the only way to satisfy this equation then we say that the vectors $v_1, v_2, \ldots, v_n$ are \textit{linearly independent}. Note that this implies that $a_i = \hat{a}_i$ for all $i$ so there is only one way to write $v$. To reiterate, a set of vectors $v_1, v_2, \ldots, v_n$ is linearly independent if and only if $a_1 v_1 + a_2 v_2 + \ldots + a_n v_n = 0$ implies that $a_1 = a_2 = \ldots = a_n = 0$. Observe that the vectors $e_1$ and $e_2$ that we looked at before are linearly independent because

$$a_1 e_1 + a_2 e_2 = (a_1, 0) + (0, a_2) = (a_1, a_2) = (0, 0)$$

immediately implies that $a_1 = a_2 = 0$. On the other hand, if we let $v_1 = (1, 2)$ and $v_2 = (3, 6)$ then

$$a_1 v_1 + a_2 v_2 = a_1 (1, 2) + a_2 (3, 6) = (0, 0)$$

also holds with $a_1 = -3$ and $a_2 = 1$. Therefore $v_1$ and $v_2$ are \textit{not} linearly independent; they're actually \textit{linearly dependent}. Of course, setting $a_1 = a_2 = 0$ would also work as it always does.

Intuitively, a set of vectors is (linearly) independent if they all point in mutually orthogonal directions. One can think of each independent vector in the set as providing another dimension to the span. In the case where $v_1 = (1, 2)$ and $v_2 = (3, 6)$ above, the vector $v_1$ and $v_2$ point in the same direction, so $v_2$ does not provide another dimension to their span (or vice versa with $v_1$). 

Linear independence is another one of those fundamental concepts of linear algebra and it comes up again and again. If its definition seems strange and its motivation unclear then that's totally OK. It will make more sense when we continue with bases.

\subsection*{Bases}

Let $V$ be a vector space. A \textit{basis} of $V$ is a linearly independent set of vectors $b_1, b_2, \ldots, b_k$ that spans $V$. For example, the set $e_1 = (1, 0, \ldots, 0), e_2 = (0, 1, \ldots, 0), \ldots, e_n = (0, 0, \ldots, 1)$ is a basis for $\mathbb{R}^n$ because it is a linearly independent set that spans $\mathbb{R}^n$ (also called the standard basis).  Similarly, $(1, 3)$ and $(7, -4)$ form a basis for $\mathbb{R}^2$ for the same reason. On the other hand, $(1, 3)$, $(7, -4)$, and $(3, 10)$ do not form a basis for $\mathbb{R}^2$ because they are linearly dependent, even though they span.

Bases are the building blocks of vector spaces. Given a vector space $V$ and a basis $b_1, b_2, \ldots, b_k$, any vector $v \in V$ can be written \textit{uniquely} as a linear combination $v = a_1 b_1 + a_2 b_2 + \ldots + a_k b_k$ of the basis vectors. This is a very important idea because it is often useful to decompose a vector into a linear combination of basis vectors when writing proofs. Here are few more important facts about bases:

\begin{enumerate}
\item A list of vectors $v_1, v_2, \ldots, v_k \in V$ is a basis of $V$ if and only if any vector $v \in V$ can be written uniquely in the form

$$v = a_1 v_1 + a_2 v_2 + \ldots + a_k v_k$$

for some $a_1, a_2, \ldots, a_k \in \mathbb{R}$.
\item Every list of spanning vectors $v_1, v_2, \ldots, v_n \in V$ can be reduced to a basis of $V$ by throwing away some of the $v_i$.

\item Every finite-dimensional vector space has a basis.

\item Every linearly independent list of vectors $v_1, v_2, \ldots, v_j \in V$ can be extended to a basis of $V$ by adding more independent vectors to the list.

\item Suppose that $V$ is a vector space and $U \subseteq V$ is a subspace. Then there exists a subspace $W \subseteq V$ such that $V = U \oplus W$.
\end{enumerate}

\subsection*{Dimension}

The last topic we'll look at in this chapter is dimension. The \textit{dimension} of a vector space $V$ is the length of a list of basis vectors of $V$. For example, $\mathbb{R}^n$ has dimension $n$ and there are $n$ vectors in the basis $e_1, e_2, \ldots, e_n$. Similarly, $\mathbb{R}^n$ has dimension 2 and there are two vectors in the basis $(1, 3)$, $(7,4)$. Note that there are infinitely many choices for a basis of a vector space $V$ and they all have the same length. We'll write $\textrm{dim} \: V$ to denote the dimension of $V$. Here are a few facts about dimensions:

\begin{enumerate}
\item If $U \subseteq V$ is a subspace of $V$ then $\textrm{dim} \: U \leq \textrm{dim} \: V$.
\item If $V$ is finite-dimensional then every list of spanning vectors in $V$ of length $\textrm{dim} \: V$ is a basis of $V$.
\item If $U_1$ and $U_2$ are subspaces of a finite-dimensional vector space then $\textrm{dim} (U_1 + U_2) = \textrm{dim} \: U_1 + \textrm{dim} \: U_2 - \textrm{dim}( U_1 \cap U_2)$.
\item If $V$ is finite-dimensional, $V = U_1 + U_2 + \ldots + U_n$, and $\textrm{dim} \: V = \textrm{dim} \: U_1 + \textrm{dim} \: U_2 + \ldots + \textrm{dim} \: U_n$ then $V = U_1 \oplus U_2 \oplus \ldots \oplus U_n$.
\end{enumerate}


\subsection*{Summary}

So there you have it! Not too complicated, right? Vector spaces are just a collection of vectors equipped with a few important properties. Now that we're comfortable working with them, we can move on to the heart of linear algebra:  linear transformations.

\end{document}