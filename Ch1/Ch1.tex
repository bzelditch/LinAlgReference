\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{epic}
\usepackage{eepic}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage{wrapfig}

\title{Chapter 1}
\author{Ben Zelditch}

\begin{document}

\maketitle

\section*{Vector Spaces}

Linear algebra is the study of linear transformations (or maps) between finite-dimensional vector spaces. Although slightly intimidating at first, this is the
right the way to define the subject. Let's break this definition down.

\subsection*{Vectors and Fields}

Before we can think about vector spaces and linear transformations, we need to define vectors. A vector $v = (v_1, v_2, \ldots, v_n)$ over a field $\mathbb{F}$ is an ordered collection of elements of $\mathbb{F}$. In other words, each $v_i$ in the vector $v$ belongs to the field $\mathbb{F}$. There are many different examples of fields, but the most common ones are the real numbers $\mathbb{R}$ and the complex numbers $\mathbb{C}$. We'll use $\mathbb{R}$ as our field in this reference because it's the most intuitive for most people, but it doesn't really matter which one you choose. The elements $v_i \in \mathbb{F}$ of the field are called scalars.

\subsection*{Vector Spaces}

Now that we've defined vectors, we can now define vector spaces. A vector space $V$ is a set of vectors $v$ over a field $\mathbb{F}$ with an addition operation and a scalar multiplication operation. This means that for any pair of vectors $x, y \in V$ in our vector space, there exists another vector $x + y \in V$ that is result of "adding" $x$ and $y$. The word "add" is in quotation marks because the addition operation doesn't have to be the usual addition operation that we normally think of, but a more general operation that satisfies the following properties:

\vskip 1mm

$\textbf{\textrm{Commutativity:}}$

$ x + y = y + x \: \textrm{for all} \: x, y \in V$

\vskip 2mm

$\textbf{\textrm{Associativity:}}$ 

$ (x + y) + z = x + (y + z) \: \textrm{for all} \: x, y, z \in V$

\vskip 2mm

$\textbf{\textrm{Additive Identity:}} $

$\: \textrm{There exists a vector} \: 0 \in V \: \textrm{such that} \: v + 0 = v \: \textrm{for all} \: v \in V$

\vskip 2mm

$\textbf{\textrm{Additive Inverse:}} $

$\: \textrm{For every} \: v \in V \: \textrm{there exists another vector} \: w \in V \: \textrm{such that} \: v + w = 0$

\vskip 2mm

$\textbf{\textrm{Multiplicative Identity:}} $

$\: \textrm{There exists a field element} \: 1 \in \mathbb{F} \: \textrm{such that} \: 1v = v \: \textrm{for all} \: v \in V$

\vskip 2mm

$\textbf{\textrm{Distributive Properties:}} $

$\: \alpha(x + y) = \alpha x + \alpha y \: \textrm{and} \: (\alpha + \beta)x = \alpha x + \beta x \: \textrm{for all} \: \alpha, \beta \in \mathbb{F} \: \textrm{and} \: x, y \in V$


\vskip 5mm
This is the most general definition for vector space addition, but clearly it isn't very useful for practical purposes. From now on we'll stick to the usual definitions of addition and scalar multiplication, taking $\mathbb{R}$ as our field and $\mathbb{R}^n$ as our vector space. So let $x = (x_1, x_2, \ldots, x_n)$ and $y = (y_1, y_2, \ldots, y_n)$ be vectors in $\mathbb{R}^n$ and let $\alpha \in \mathbb{R}$ be a real number. Then

$$x + y = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \: \textrm{and} \: \alpha x = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n).$$

The additive identity $0 \in \mathbb{R}^n$ is $(0, 0 \ldots, 0)$ and the multiplicative identity $1 \in \mathbb{R}$ is 1, of course. 

Before moving on, there are a few properties of vector spaces that are basic, but fundamental, and we won't prove them here. The first is that the additive identity $0$ is unique. The second is that every vector $x$ in the vector space has a unique additive inverse, denoted $-x$. The third is that $-1(x) = -x$ for all $x$ in the vector space.

Another important concept is the addition of two vector spaces. If $U$ and $V$ are two vector spaces, then we define the \textit{sum} of $U$ and $V$ to be $U + V = \{ u + v : \: u \in U, v \in V \}$. For example, if $U = \{ (x_1, 0) : x_1 \in \mathbb{R} \}$ and $V = \{ (0, x_2) : x_2 \in \mathbb{R} \}$, then $U + V = \{ (x_1, 0) + (0, x_2) : x_1 \in \mathbb{R}, x_2 \in \mathbb{R} \} = \{ (x_1, x_2) : x_1 \in \mathbb{R}, x_2 \in \mathbb{R} \} = \mathbb{R}^2$. Observe from this example that we can now decompose a vector space into the sum of two other vector spaces. This is a very useful tool that we'll use later. Finally, we define the \textit{direct sum} of two vector spaces. We say that a vector $W$ is the direct sum of two vector spaces $U$ and $V$ if and only if $W = U + V$ \textit{and} $U \cap V = \{0\}$. This is written as $W = U \oplus V$. In fact, the example we just looked at is an example of a direct sum! Therefore we can write $\mathbb{R}^2 = \{ (x_1, 0) : x_1 \in \mathbb{R} \} \oplus \{ (0, x_2) : x_2 \in \mathbb{R} \}$. More on this later.

\subsection*{Subspaces}

A subspace $U$ of the vector space $V$ is a subset $U \subseteq V$ that is itself a vector space. For example, if $V = \mathbb{R}^2$ then one possible subspace of $V$ (as we saw before) is $U = \{ (x_1, 0) : x_1 \in \mathbb{R} \}$. It isn't hard to see that $U$ satisfies all of the vector space properties listed above, so it definitely qualifies as a subspace of $V$. 

\subsection*{Summary}

So there you have it! Not too complicated, right? Vector spaces are just a collection of vectors equipped with a few important properties. Now that we're comfortable working with them, we can move on to the heart of linear algebra:  linear transformations.

\end{document}